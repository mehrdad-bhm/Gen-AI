{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehrdad-bhm/Gen-AI/blob/main/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZB_KLeXYQ7Z"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain_community pypdf chromadb langchain_huggingface openai tiktoken huggingface_hub accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtcwrQ-iYNsr"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.schema import Document\n",
        "from langchain.vectorstores.chroma import Chroma\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
        "import os\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"xxx\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"xxx\""
      ],
      "metadata": {
        "id": "_nE9VrMHS3XO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjfLu_Xc-U0p"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = r\"data\"\n",
        "CHROMA_PATH = \"chroma\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbishiiUYNss"
      },
      "outputs": [],
      "source": [
        "def load_documents():\n",
        "    document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
        "    return document_loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwXQm1YBYNsu"
      },
      "outputs": [],
      "source": [
        "def split_text(documents: list[Document]):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=400,\n",
        "        chunk_overlap=100,\n",
        "        length_function=len,\n",
        "        add_start_index=True,\n",
        "    )\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klb5nKDFYNsu"
      },
      "outputs": [],
      "source": [
        "def save_to_chroma(chunks: list[Document]):\n",
        "    if os.path.exists(CHROMA_PATH):\n",
        "        shutil.rmtree(CHROMA_PATH)\n",
        "\n",
        "    db = Chroma.from_documents(\n",
        "        chunks, HuggingFaceEmbeddings(), persist_directory=CHROMA_PATH\n",
        "    )\n",
        "    db.persist()\n",
        "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-lqJeQkYNsv"
      },
      "outputs": [],
      "source": [
        "def generate_data_store():\n",
        "    documents = load_documents()\n",
        "    chunks = split_text(documents)\n",
        "    save_to_chroma(chunks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EgZ5VQ_YNsv"
      },
      "outputs": [],
      "source": [
        "generate_data_store()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibjaYudXYNsw"
      },
      "outputs": [],
      "source": [
        "query_text = \"Explain how to discard structure results\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LYcHtBHYNsw"
      },
      "outputs": [],
      "source": [
        "PROMPT_TEMPLATE = \"\"\"\n",
        "Answer the question based only on the following context:\n",
        "\n",
        "{context}\n",
        "\n",
        "---\n",
        "\n",
        "Answer the question based on the above context: {question}\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dejED-aYNsw"
      },
      "outputs": [],
      "source": [
        "db = Chroma(persist_directory=CHROMA_PATH, embedding_function=HuggingFaceEmbeddings())\n",
        "\n",
        "results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
        "if len(results) == 0 or results[0][1] < 0.1:\n",
        "    print(f\"Unable to find matching results.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjFPGz4nYNsw"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QnX25P8YNsw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ac7d5bb-5cd4-48b3-90cd-7864dde5b2e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: \n",
            "Answer the question based only on the following context:\n",
            "\n",
            "simulation result fields (volume fields) from the memory. \n",
            " \n",
            "Selecting Discard Volume Files eliminates only the simulation result fields (volume \n",
            "fields) that are currently loaded  from the memory. The structure displayed in the \n",
            "visualization area is not deleted.\n",
            "\n",
            "---\n",
            "\n",
            "into GeoDict (green dot), or not (red dot). \n",
            "The Structure model is identified by File Name, Description, Voxel Count, Voxel \n",
            "Length, Domain Size and the internally hashed Structure ID number. A structure \n",
            "generated, e.g., with FiberGeo, GrainGeo, PaperGeo, WeaveGeo, GridGeo, or \n",
            "PleatGeo, with the same parameters and the same random seed, has always the same\n",
            "\n",
            "---\n",
            "\n",
            "GeoDict Graphical User Interface â€“ Menu bar \n",
            "GeoDict 2024 User Guide  17 \n",
            "DISCARD STRUCTURE OR SIMULATION RESULTS  \n",
            "Selecting Discard Structure and Volume Files  makes the structure, and all \n",
            "simulation result fields available for this structure, disappear from memory and from \n",
            "the Visualization area. The initial  GeoDict splash screen appears in the  visualization \n",
            "area instead.\n",
            "\n",
            "---\n",
            "\n",
            "Answer the question based on the above context: Explain how to discard structure results\n",
            "\n"
          ]
        }
      ],
      "source": [
        "context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
        "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
        "prompt = prompt_template.format(context=context_text, question=query_text)\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOS29yf8pR-q"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsI5CnPVvwxY"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import ChatHuggingFace\n",
        "\n",
        "\n",
        "#1 using openai\n",
        "'''\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "llm = ChatOpenAI(\n",
        "    model_name=\"gpt-3.5-turbo\",\n",
        "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "    max_tokens=512,\n",
        "    temperature=0.7\n",
        ")'''\n",
        "\n",
        "#2 using hf pipline\n",
        "'''\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "llm = HuggingFacePipeline.from_model_id(\n",
        "    # model_id=\"gpt2\",\n",
        "    model_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs=dict(\n",
        "        max_new_tokens=512,\n",
        "        do_sample=False,\n",
        "        repetition_penalty=1.03,\n",
        "    ),\n",
        ")'''\n",
        "\n",
        "#3 using hf endpoint\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "llm = HuggingFaceEndpoint(repo_id=\"HuggingFaceH4/zephyr-7b-beta\")\n",
        "\n",
        "\n",
        "model = ChatHuggingFace(llm=llm)\n",
        "response_text = model.predict(prompt)\n",
        "sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
        "formatted_response = f\"Response: {response_text}\\n\\nSources: {sources}\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(formatted_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FzlEt_tRT4p",
        "outputId": "0f28f103-1906-4189-82b8-d110c49585ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: Based on the given context, to discard both structure and simulation result fields in GeoDict, you need to follow these steps:\n",
            "\n",
            "1. Open the GeoDict software and load the structure you want to discard by selecting it in the Project tree or by browsing in the Open dialog.\n",
            "\n",
            "2. In the menu bar, go to \"File\" and select \"Discard Structure and Volume Files\". This will delete both the structure and all simulation result fields associated with\n",
            "\n",
            "Sources: ['data/Base Reference 2024.pdf', 'data/Base Reference 2024.pdf', 'data/Base Reference 2024.pdf']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}